```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```


## Exercise 10.1

In this assignment, you will use R to understand and apply document classification and supervised scaling using R and **quanteda**.

We will start with a classic computer science dataset of movie reviews, [(Pang and Lee 2004)](http://www.cs.cornell.edu/home/llee/papers/cutsent.pdf). The movies corpus has an attribute `Sentiment` that labels each text as either `pos` or `neg` according to the original imdb.com archived newspaper review star rating.  We will begin by examining the conditional probabilities at the word level.

```{r}
library("tm")           # for text mining
library("quanteda")     # for text analysis
library("readtext")     # for reading text
library("quanteda.corpora")

#devtools::install_github("quanteda/quanteda.corpora")
set.seed(1234)  # use this just before the command below
moviesShuffled <- corpus_sample(data_corpus_movies, size = 2000)
```

a) Make a dfm from the shuffled corpus, and make training labels. In this case, we are using 1500 training labels, and leaving the remaining 500 unlabelled to use as a test set. We will also trim the dataset to remove rare features. 

```{r}
# dfm_sample(x, size = ifelse(margin == "documents", ndoc(x), nfeat(x)),
#  replace = FALSE, prob = NULL, margin = c("documents", "features"))
dfm.mov <- dfm(moviesShuffled, remove = stopwords("en"), remove_punct = TRUE, 
           remove_symbols = TRUE)

dfm.mov <- dfm_trim(dfm.mov, remove_punct = TRUE, min_termfreq = 5, remove_symbols=TRUE)
dfm.mov.train <- dfm_sample(dfm.mov, size = 1500, margin = "documents")      
train <- which(docnames(dfm.mov.train) == docnames(dfm.mov.train))
dfm.mov.test <- dfm.mov[-train]

```

b) Run the training and testing commands of the Naive Bayes classifier, and compare the predictions for the documents with the actual document labels for the test set using a confusion matrix.
```{r}
# TODO ask do we remove sentiment here? 
# do we set seed? 

library(caret)

post <- docvars(dfm.mov.train, field="Sentiment")

nb.mov <- textmodel_nb(x = dfm.mov.train, post)
nb.pred <- predict(nb.mov, type="class")

caret::confusionMatrix(data = relevel(as.factor(nb.pred), ref = "neg"),
                       reference = relevel(as.factor(post), ref = "neg"))

```


c) Compute the following statistics for the last classification:

Use this code for starters, and note that it returns something that you can use to compute \(F1\).

```{r}
precrecall <- function(mytable, verbose=TRUE) {
    truePositives <- mytable[1,1]
    falsePositives <- sum(mytable[1,]) - truePositives
    falseNegatives <- sum(mytable[,1]) - truePositives
    precision <- truePositives / (truePositives + falsePositives)
    recall <- truePositives / (truePositives + falseNegatives)
    if (verbose) {
        print(mytable)
        cat("\n precision =", round(precision, 2), 
            "\n    recall =", round(recall, 2), "\n")
    }
    invisible(c(precision, recall))
}

# positive class is "neg"
precrecall(table(nb.pred, post))

# positive class is "pos"
precrecall(table(nb.pred, post)[2:1, 2:1])     # this is the same thing..

caret::confusionMatrix(table(nb.pred, post)[2:1, 2:1])
```
    
Hint: Computing precision and recall is not the same if we are considering the "true positive" to be predicting positive for a true positive, versus predicting negative for a true negative.  Since the factors of `Sentiment` are ordered alphabetically, and since the table command puts lower integer codes for factors first, `movtable` by default puts the (1,1) cell as the case of predicting negative reviews as the "true positive", not predicting positive reviews.  To get the positive-postive prediction you will need to reverse index it, e.g. `movTable[2:1, 2:1]`.

d) Extract the posterior class probabilities of the words `good` and `great`. Do the results confirm your previous finding? Clue: look at the documentation for `textmodel_nb()` for how to extract the posterior class probabilities.

```{r}

# had trouble with this earlier. 
nb.mov$PcGw[,c("good", "great")]

```


## Exercise 10.2

```{r}
data(data_corpus_movies, package = "quanteda.corpora")
```

a) Load the movies dataset from `quanteda.corpora` (install from GitHub with the provided code if necessary). Then, shuffle the dataset, and take a random sample of 500 of the movie reviews as your "reference" texts. As reference scores, set the ones that are positive to a reference value of +1, and the negative reviews to a value of -1.

```{r}

set.seed(1234)

moviesShuffled2 <- corpus_sample(data_corpus_movies, size = 2000)

dfm.mov2 <- dfm(moviesShuffled2, remove = stopwords("en"), 
                remove_punct = TRUE, remove_symbols = TRUE)

dfm.mov2 <- dfm_trim(dfm.mov2, min_termfreq = 2, min_docfreq = 2, 
                     remove_punct = TRUE, remove_symbols = TRUE)

dfm.mov.ref <- dfm_sample(dfm.mov2, size = 500, margin = "documents")

refscore <- rep(0, length(docnames(dfm.mov.ref)))

refscore[docvars(dfm.mov.ref, field="Sentiment") == "neg"] <- -1
refscore[docvars(dfm.mov.ref, field="Sentiment") == "pos"] <- 1

```

b) Score the remaining movie reviews, and predict their "positive-negative" rating using Wordscores. Remember to first create a document-feature matrix. You may want to stem the features here.

```{r}

# create not-ref dfm
ref <- which(docnames(dfm.mov.ref) == docnames(dfm.mov.ref))
dfm.mov.virgin <- dfm.mov[-ref]

# clean up and stem
# why stem here and not there? 
virginscore <- rep(0, length(docnames(dfm.mov.virgin)))
dfm.mov.virgin <- dfm_trim(dfm.mov.virgin, min_termfreq = 2, min_docfreq = 2, 
                     remove_punct = TRUE, remove_symbols = TRUE)
dfm.mov.virgin <- dfm_wordstem(dfm.mov.virgin)

# create y
virginscore[docvars(dfm.mov.virgin, field="Sentiment") == "neg"] <- -1
virginscore[docvars(dfm.mov.virgin, field="Sentiment") == "pos"] <- 1

# run model
mov.wc <- textmodel_wordscores(x = dfm.mov.ref, y = refscore)
mov.notref.pred <- predict(mov.wc, newdata = dfm.mov.virgin)


# more elegant to mutate dfm itself with a new docvars stating "virgin" or "reference"
# --------- from solution --------
# create a DFM
# moviesDfm <- dfm(moviesShuffled, stem = TRUE)

# fit the wordscores model
# ws <- textmodel_wordscores(moviesDfm, docvars(moviesShuffled, "refscore"))

# predicting the scale for the other features
# preds <- predict(ws, moviesDfm[docvars(moviesShuffled, "set") == "virgin", ])

```


c) From the results of b, compare the values using `boxplot()` for the categories of their rater assigned positivity or negativity.  Describe the resulting pattern. Look for examples of positive reviews that are predicted to be negative and vice versa. Why do you think the model failed in those cases?

```{r}
boxplot(mov.notref.pred ~ docvars(dfm.mov.virgin, "Sentiment"), ylab = "Raw wordscore") 

# why is the boundary -0.05? 
false_negative <- which(preds < -0.05 & docvars(dfm.mov.virgin)=="pos")
length(false_negative)
texts(data_corpus_movies)[sample(false_negative, 2)]

```

**Model is not as accurate as we imagine. The ref text is selected at random, so may not have the full body of text needed for virgin text. As noted, 4105 features in newdata are not used in prediction. The text in reference text may sound very bad, or may not have edge cases. When predicting on virgin text (when virgin text is scored based on ref), this leads model to believe that movies are not liked.**

```{r}
false_positive <- which(preds > 0.05 & docvars(moviesShuffled, "Sentiment")[501:2000]=="neg")
length(false_positive)
# TODO ask why no false positive?
# why is my answer the same as solution?
```

**I'm sure the model overfitted. **

## Exercise 10.3

In this part of the assignment, you will use R to understand and apply unsupervised document scaling. Use the `data_corpus_irishbudget2010` in **quanteda** for this.

```{r}

irish_dfm <- dfm(data_corpus_irishbudget2010, stem = TRUE, 
                 remove = stopwords("en"), remove_punct = TRUE)
```

a) Fit a wordfish model of all the documents in this corpus. Apply any required preprocessing steps first. Use the `textplot_scale1d` function to visualize the result. (You may want to use the advanced options of this function to get a better plot than just the default one.) What do you learn about what the dimension is capturing? You can use wikipedia to learn about the Irish parties involved in this debate to help you answer this question.

```{r}
irish_dfm <- dfm_trim(x = irish_dfm, remove_punct=TRUE, remove_numbers=TRUE, remove_symbols = TRUE,
                      remove = stopwords("en"), min_termfreq=5)
wfFit <- textmodel_wordfish(x = irish_dfm)
me_doc <- as.data.frame(cbind(wfFit$docs, wfFit$theta))
# theta is the politician's ideological position. TODO how to know if 1 = left or 2 = left?

textplot_scale1d(wfFit, groups=docvars(data_corpus_irishbudget2010)$party)
# Err. Not very true. Lab is the centre-left, Fine Gael (FG) should be centre right, Sinn FÃ©in (SF) should be centre left, 
```

**Morgan, Arthur (SF) has the highest estimated document fixed effect (alpha) at 0.97481242350511**

b) Plot the wordfish "Eiffel Tower" plot (as in Figure 2 of Slapin and Proksch 2008), from the wordfish object. You can do this using the `textplot_scale1d` function or (even better) using the more advanced code we used in the lecture.

```{r}
textplot_scale1d(wfFit, margin = c("features"))

# or an ugly version
plot(wfFit$beta, wfFit$psi)
text(x=wfFit$beta, y=wfFit$psi, labels=wfFit$features, col = c(2,9) )
```


c) Plot the log of the length in tokens of each text against the alpha-hat from `wfFit`. What does the relationship indicate?

```{r}
plot(log(ntoken(irish_dfm)), wfFit$alpha)
# alhpa_hat = estimated document fixed effects
# number of tokens for each document is increasing in its fixed effects. 
```


d) Plot the log of the frequency of the top most frequent 1000 words against the same psi-hat values from `wfit`, and describe the relationship.

```{r}
# log(frequency(which.max(irish_dfm))))
```

